{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw text classification with BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load for Jupyter Notebook\n",
    "import sys\n",
    "sys.path.append('/home/elenaruiz/Documents/TFG/FNC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim import corpora, models\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.utils import io\n",
    "from src.fake_news_detector.core.nlp import clean_text as ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load dataset (tmp.json)\n",
    "\n",
    "The file `tmp.json` contains all raw data that has to be tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = io.read_json_file('/home/elenaruiz/Documents/TFG/FNC/src/data/tmp.json')\n",
    "df = pd.DataFrame(data=articles['articles']) # Put in pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fake', 'subtitle', 'text', 'title', 'url'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Join and clean raw text\n",
    "\n",
    "Tokenize in words. For each article store list of Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "i = 0\n",
    "for _, row in df.iterrows():\n",
    "    x = ct.clean_text_by_word(row['title'], True)\n",
    "    y = ct.clean_text_by_word(row['subtitle'], True)\n",
    "    z = []\n",
    "    for sent in row['text']:\n",
    "        z += ct.clean_text_by_word(sent, True)\n",
    "    corpus.append(x + y + z)\n",
    "    i = i + 1\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store in a dataframe\n",
    "documents = pd.DataFrame(data={'corpus': corpus, 'label': df['fake']*1 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[they, find, corpse, vegetarian, restaurant, B...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[switzerland, warn, authorize, extradition, po...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[navarre, censor, Songs, Amaral, Shakira, song...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[a, woman, pretend, blind, years, greet, peopl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[arrested, ejaculate, boss, coffee, last, four...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              corpus  label\n",
       "0  [they, find, corpse, vegetarian, restaurant, B...      1\n",
       "1  [switzerland, warn, authorize, extradition, po...      1\n",
       "2  [navarre, censor, Songs, Amaral, Shakira, song...      1\n",
       "3  [a, woman, pretend, blind, years, greet, peopl...      1\n",
       "4  [arrested, ejaculate, boss, coffee, last, four...      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Bag of Words\n",
    "Create dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(documents['corpus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 A\n",
      "1 Asians\n",
      "2 Bangkok\n",
      "3 Daily\n",
      "4 Errors\n",
      "5 Inpathom\n",
      "6 Khaosod\n",
      "7 Mail\n",
      "8 October\n",
      "9 Prasit\n",
      "10 Restaurant\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se quitan los menos usados\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Vectorizar el contenido\n",
    "Realizar proceso de doc2bow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 1),\n",
       " (4, 2),\n",
       " (11, 7),\n",
       " (15, 1),\n",
       " (17, 2),\n",
       " (22, 1),\n",
       " (25, 1),\n",
       " (30, 2),\n",
       " (35, 3),\n",
       " (40, 1),\n",
       " (41, 1),\n",
       " (52, 1),\n",
       " (64, 2),\n",
       " (67, 1),\n",
       " (70, 1),\n",
       " (77, 1),\n",
       " (81, 1),\n",
       " (83, 1),\n",
       " (89, 1),\n",
       " (99, 1),\n",
       " (101, 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in documents['corpus']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 2 (\"find\") appears 1 time.\n",
      "Word 5 (\"he\") appears 1 time.\n",
      "Word 7 (\"like\") appears 1 time.\n",
      "Word 8 (\"make\") appears 2 time.\n",
      "Word 13 (\"say\") appears 4 time.\n",
      "Word 16 (\"time\") appears 2 time.\n",
      "Word 19 (\"accord\") appears 2 time.\n",
      "Word 23 (\"case\") appears 3 time.\n",
      "Word 32 (\"seem\") appears 1 time.\n",
      "Word 46 (\"it\") appears 4 time.\n",
      "Word 50 (\"explain\") appears 1 time.\n",
      "Word 51 (\"many\") appears 1 time.\n",
      "Word 52 (\"people\") appears 1 time.\n",
      "Word 54 (\"this\") appears 1 time.\n",
      "Word 55 (\"years\") appears 2 time.\n",
      "Word 60 (\"could\") appears 1 time.\n",
      "Word 62 (\"point\") appears 1 time.\n",
      "Word 63 (\"that\") appears 1 time.\n",
      "Word 64 (\"another\") appears 2 time.\n",
      "Word 68 (\"even\") appears 1 time.\n",
      "Word 69 (\"take\") appears 3 time.\n",
      "Word 75 (\"European\") appears 2 time.\n",
      "Word 81 (\"we\") appears 2 time.\n",
      "Word 91 (\"go\") appears 1 time.\n",
      "Word 95 (\"still\") appears 1 time.\n",
      "Word 98 (\"become\") appears 1 time.\n",
      "Word 100 (\"next\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE\n",
    "bow_corpus[30]\n",
    "bow_doc_30 = bow_corpus[30]\n",
    "\n",
    "for i in range(len(bow_doc_30)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_30[i][0], \n",
    "                                                     dictionary[bow_doc_30[i][0]], \n",
    "                                                     bow_doc_30[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "Realizar proceso de ordenar por frecuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.1433809580681048),\n",
      " (1, 0.1263779958704988),\n",
      " (2, 0.4246240358860827),\n",
      " (3, 0.11563340754452661),\n",
      " (4, 0.11905796208001264),\n",
      " (5, 0.13030389088330102),\n",
      " (6, 0.08080304837609287),\n",
      " (7, 0.10615600897152068),\n",
      " (8, 0.07060398679259744),\n",
      " (9, 0.14825911136998027),\n",
      " (10, 0.13030389088330102),\n",
      " (11, 0.7412955568499014),\n",
      " (12, 0.29651822273996054),\n",
      " (13, 0.059859398466625226),\n",
      " (14, 0.1263779958704988),\n",
      " (15, 0.11563340754452661),\n",
      " (16, 0.08080304837609287)]\n"
     ]
    }
   ],
   "source": [
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. LDA con BOW\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.053*\"say\" + 0.040*\"I\" + 0.037*\"police\" + 0.035*\"right\" + 0.028*\"people\" + 0.024*\"know\" + 0.024*\"another\" + 0.022*\"country\" + 0.021*\"call\" + 0.021*\"case\"\n",
      "Topic: 1 \n",
      "Words: 0.044*\"use\" + 0.037*\"already\" + 0.036*\"point\" + 0.033*\"public\" + 0.033*\"they\" + 0.029*\"come\" + 0.027*\"consider\" + 0.025*\"way\" + 0.025*\"many\" + 0.024*\"in\"\n",
      "Topic: 2 \n",
      "Words: 0.045*\"say\" + 0.041*\"police\" + 0.036*\"euros\" + 0.033*\"would\" + 0.028*\"first\" + 0.027*\"in\" + 0.024*\"use\" + 0.022*\"new\" + 0.021*\"find\" + 0.021*\"two\"\n",
      "Topic: 3 \n",
      "Words: 0.049*\"in\" + 0.045*\"use\" + 0.041*\"make\" + 0.039*\"first\" + 0.034*\"open\" + 0.033*\"euros\" + 0.031*\"go\" + 0.029*\"since\" + 0.028*\"new\" + 0.026*\"it\"\n",
      "Topic: 4 \n",
      "Words: 0.048*\"would\" + 0.045*\"time\" + 0.040*\"could\" + 0.035*\"end\" + 0.033*\"say\" + 0.029*\"years\" + 0.026*\"company\" + 0.025*\"case\" + 0.023*\"European\" + 0.023*\"want\"\n",
      "Topic: 5 \n",
      "Words: 0.053*\"it\" + 0.030*\"time\" + 0.028*\"take\" + 0.026*\"they\" + 0.025*\"years\" + 0.024*\"a\" + 0.024*\"go\" + 0.023*\"people\" + 0.022*\"say\" + 0.021*\"he\"\n",
      "Topic: 6 \n",
      "Words: 0.049*\"take\" + 0.038*\"say\" + 0.037*\"make\" + 0.031*\"people\" + 0.025*\"already\" + 0.022*\"years\" + 0.021*\"place\" + 0.021*\"The\" + 0.020*\"they\" + 0.020*\"must\"\n",
      "Topic: 7 \n",
      "Words: 0.060*\"first\" + 0.043*\"this\" + 0.041*\"new\" + 0.036*\"de\" + 0.035*\"case\" + 0.032*\"many\" + 0.030*\"country\" + 0.029*\"report\" + 0.029*\"it\" + 0.027*\"we\"\n",
      "Topic: 8 \n",
      "Words: 0.087*\"Spain\" + 0.063*\"Spanish\" + 0.039*\"it\" + 0.035*\"in\" + 0.032*\"say\" + 0.028*\"time\" + 0.026*\"European\" + 0.026*\"last\" + 0.022*\"explain\" + 0.020*\"state\"\n",
      "Topic: 9 \n",
      "Words: 0.058*\"Spanish\" + 0.039*\"new\" + 0.039*\"European\" + 0.034*\"government\" + 0.027*\"point\" + 0.023*\"year\" + 0.023*\"change\" + 0.020*\"in\" + 0.020*\"go\" + 0.019*\"like\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/susanli2016/NLP-with-Python/blob/master/LDA_news_headlines.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
